# app.py â€” DEMO ì •í™•ë„ = ìƒí’ˆë³„ RÂ²ì˜ íŒë§¤ëŸ‰ ê°€ì¤‘ í‰ê· 
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path

st.set_page_config(page_title="Randy's Donuts ìˆ˜ìš”ì˜ˆì¸¡ DEMO", layout="wide")

# -----------------------------
# DEMO ê²½ë¡œ / íŒŒì¼ ë§¤í•‘
# -----------------------------
BASE_DIR = Path(r"C:\Users\lalav\OneDrive\ë°”íƒ• í™”ë©´\DEMO")
FILES = {
    "ì œì£¼ì• ì›”ì ": {
        "pos": BASE_DIR / "aewol_POS.csv",
        "pred": BASE_DIR / "aewol_PRED.csv",
        "pred_kind": "HGBR"
    },
    "ë¶€ì‚°ê´‘ì•ˆë¦¬ì ": {
        "pos": BASE_DIR / "gwangan_POS.csv",
        "pred": BASE_DIR / "gwangan_PRED.csv",
        "pred_kind": "CatBoost log1p"
    },
    "ìˆ˜ì›íƒ€ì„ë¹Œë¦¬ì§€ì ": {
        "pos": BASE_DIR / "suwon_POS.csv",
        "pred": BASE_DIR / "suwon_PRED.csv",
        "pred_kind": "CatBoost"
    },
    "ì—°ë‚¨ì ": {
        "pos": BASE_DIR / "yeonnam_POS.csv",
        "pred": BASE_DIR / "yeonnam_PRED.csv",
        "pred_kind": "CatBoost log1p"
    },
}

# -----------------------------
# ìœ í‹¸: ë¡œë”©/ì •ê·œí™”/ì§€í‘œ
# -----------------------------
@st.cache_data
def load_pos(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    # ë‚ ì§œ ì •ê·œí™”
    if "ì¼ì" in df.columns:
        df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    elif "ë‚ ì§œ" in df.columns:
        df["ì¼ì"] = pd.to_datetime(df["ë‚ ì§œ"])
    else:
        raise ValueError(f"POS ë‚ ì§œ ì»¬ëŸ¼(ì¼ì/ë‚ ì§œ) ì—†ìŒ: {path}")
    # ìˆ˜ëŸ‰ ì •ê·œí™”
    if "ìˆ˜ëŸ‰" not in df.columns:
        for c in ["íŒë§¤ìˆ˜ëŸ‰", "ì‹¤ì œíŒë§¤", "ì‹¤ì œ", "qty", "QTY"]:
            if c in df.columns:
                df["ìˆ˜ëŸ‰"] = df[c]
                break
        if "ìˆ˜ëŸ‰" not in df.columns:
            raise ValueError(f"POS ìˆ˜ëŸ‰ ì»¬ëŸ¼ ì—†ìŒ: {path}")
    # ìƒí’ˆëª… ë³´ì •
    if "ìƒí’ˆëª…" not in df.columns:
        for c in ["product_name", "ìƒí’ˆ"]:
            if c in df.columns:
                df["ìƒí’ˆëª…"] = df[c]
                break
    return df

def _looks_log_scale(df: pd.DataFrame, store_name: str) -> bool:
    header = " ".join(map(str, df.columns)).lower()
    return ("log" in header) or ("log1p" in header) or ("ì—°ë‚¨" in store_name and "log" in header)

@st.cache_data
def load_pred(path: Path, store_name: str = "", model_name: str = "") -> pd.DataFrame:
    df = pd.read_csv(path)
    # ë‚ ì§œ ì •ê·œí™”
    if "ë‚ ì§œ" in df.columns:
        df["ë‚ ì§œ"] = pd.to_datetime(df["ë‚ ì§œ"])
    elif "ì¼ì" in df.columns:
        df["ë‚ ì§œ"] = pd.to_datetime(df["ì¼ì"])
    else:
        raise ValueError(f"PRED ë‚ ì§œ ì»¬ëŸ¼(ë‚ ì§œ/ì¼ì) ì—†ìŒ: {path}")
    # ì˜ˆì¸¡ìˆ˜ëŸ‰ ì •ê·œí™”
    if "ì˜ˆì¸¡ìˆ˜ëŸ‰" not in df.columns:
        for c in ["pred", "ì˜ˆì¸¡", "prediction", "ì˜ˆì¸¡(ì‹ )"]:
            if c in df.columns:
                df["ì˜ˆì¸¡ìˆ˜ëŸ‰"] = df[c]
                break
        if "ì˜ˆì¸¡ìˆ˜ëŸ‰" not in df.columns:
            raise ValueError(f"PRED ì˜ˆì¸¡ìˆ˜ëŸ‰ ì»¬ëŸ¼ ì—†ìŒ: {path}")
    # ìƒí’ˆëª… ë³´ì •
    if "ìƒí’ˆëª…" not in df.columns:
        for c in ["product_name", "ìƒí’ˆ"]:
            if c in df.columns:
                df["ìƒí’ˆëª…"] = df[c]
                break
    # âœ… ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³µì› (ë‹¨, ì‹¤ì œë¡œ log ê°’ì¼ ë•Œë§Œ ë³µì›)
    if _looks_log_scale(df, store_name) or ("log" in model_name.lower()):
        # í‰ê· ì´ë‚˜ ìµœëŒ€ê°’ì´ log ë²”ìœ„(<=20) ì•ˆì¼ ë•Œë§Œ expm1 ìˆ˜í–‰
        if df["ì˜ˆì¸¡ìˆ˜ëŸ‰"].max() < 20 and df["ì˜ˆì¸¡ìˆ˜ëŸ‰"].mean() < 10:
            df["ì˜ˆì¸¡ìˆ˜ëŸ‰"] = np.expm1(np.clip(df["ì˜ˆì¸¡ìˆ˜ëŸ‰"].astype(float), a_min=-20, a_max=None))
    return df

def align_common_dates(pos_df: pd.DataFrame, pred_df: pd.DataFrame):
    common = sorted(set(pos_df["ì¼ì"]) & set(pred_df["ë‚ ì§œ"]))
    return (
        pos_df[pos_df["ì¼ì"].isin(common)].copy(),
        pred_df[pred_df["ë‚ ì§œ"].isin(common)].copy(),
    )

def normalize_grain_pos(df: pd.DataFrame) -> pd.DataFrame:
    cols = [c for c in ["ìƒí’ˆëª…"] if c in df.columns]
    return df.groupby(["ì¼ì"] + cols, as_index=False)["ìˆ˜ëŸ‰"].sum()

def normalize_grain_pred(df: pd.DataFrame) -> pd.DataFrame:
    cols = [c for c in ["ìƒí’ˆëª…"] if c in df.columns]
    out = df.groupby(["ë‚ ì§œ"] + cols, as_index=False)["ì˜ˆì¸¡ìˆ˜ëŸ‰"].sum()
    out["ì˜ˆì¸¡ìˆ˜ëŸ‰"] = out["ì˜ˆì¸¡ìˆ˜ëŸ‰"].clip(lower=0)  # ìŒìˆ˜ ë°©ì§€
    return out

def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    if len(y_true) == 0:
        return np.nan
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - ss_res / ss_tot if ss_tot != 0 else np.nan

def r2_weighted_by_sales(df: pd.DataFrame) -> float:
    """ìƒí’ˆë³„ RÂ² ê³„ì‚° í›„, ê° ìƒí’ˆì˜ ì‹¤ì œíŒë§¤ëŸ‰ í•©ê³„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ê°€ì¤‘ í‰ê· """
    if "ìƒí’ˆëª…" not in df.columns:
        return np.nan
    parts, weights = [], []
    for _, g in df.groupby("ìƒí’ˆëª…"):
        r2 = r2_score(g["ì‹¤ì œíŒë§¤ëŸ‰"], g["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"])
        w = float(g["ì‹¤ì œíŒë§¤ëŸ‰"].sum())
        if not np.isnan(r2) and w > 0:
            parts.append(r2)
            weights.append(w)
    if not parts:
        return np.nan
    return float(np.average(parts, weights=weights))

def filter_sparse_items_for_metric(df: pd.DataFrame, min_days: int = 3) -> pd.DataFrame:
    """ì§€í‘œ ê³„ì‚°ì—ì„œ ìµœì†Œ ì¼ìˆ˜ ë¯¸ë§Œ SKU ì œì™¸(í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ)"""
    if "ìƒí’ˆëª…" not in df.columns:
        return df
    days = df.groupby("ìƒí’ˆëª…")["ë‚ ì§œ"].nunique()
    keep = days[days >= min_days].index
    return df[df["ìƒí’ˆëª…"].isin(keep)]

def to_kor_date(d: pd.Timestamp) -> str:
    return d.strftime("%y-%m-%d")

# -----------------------------
# ìŠ¤íƒ€ì¼
# -----------------------------
st.markdown(
    """
    <style>
    .title-bar {font-size:28px; font-weight:700; margin-bottom:12px;}
    .card {background:#fff; border-radius:12px; padding:12px 14px; border:1px solid #eee;}
    .muted {color:#666;}
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<div class="title-bar">ğŸ© Randy\'s Donuts Â· ìˆ˜ìš”ì˜ˆì¸¡ ì‹œìŠ¤í…œ</div>', unsafe_allow_html=True)

tab_names = list(FILES.keys())
tabs = st.tabs(tab_names)

# -----------------------------
# íƒ­ ë Œë”ë§
# -----------------------------
for tab_name, tab in zip(tab_names, tabs):
    with tab:
        conf = FILES[tab_name]
        model_name = conf.get("pred_kind", "")
        colL, colR = st.columns([1.45, 1.0], gap="large")

        # ë°ì´í„° ë¡œë“œ ë° ê³µí†µì¼ì ì •ë ¬
        pos = load_pos(conf["pos"])
        pred = load_pred(conf["pred"], store_name=tab_name, model_name=model_name)
        pos, pred = align_common_dates(pos, pred)

        st.markdown(f"**ì§€ì :** {tab_name}  Â·  ëª¨ë¸: {model_name}")

        # ----------------- ì¢Œ: ê²€ì¦ íŒ¨ë„ -----------------
        with colL:
            st.subheader("ì¡°íšŒê¸°ê°„ Â· ìƒí’ˆì½”ë“œ / ê²€ì¦")

            c1, c2, c3, c4, c5 = st.columns([0.6, 0.6, 0.7, 1.2, 0.5])
            min_d = pos["ì¼ì"].min()
            max_d = pos["ì¼ì"].max()

            from_d = c1.date_input(
                "ì¡°íšŒ ì‹œì‘ì¼",
                min_d.date() if pd.notna(min_d) else datetime.today().date(),
                key=f"{tab_name}_from_date",
            )
            to_d = c2.date_input(
                "ì¡°íšŒ ì¢…ë£Œì¼",
                max_d.date() if pd.notna(max_d) else datetime.today().date(),
                key=f"{tab_name}_to_date",
            )

            item_list = (
                sorted(pos["ìƒí’ˆëª…"].dropna().unique().tolist())
                if "ìƒí’ˆëª…" in pos.columns
                else ["(ì „ì²´)"]
            )
            item_sel = c3.selectbox(
                "ìƒí’ˆëª…",
                options=["(ì „ì²´)"] + item_list,
                index=0,
                key=f"{tab_name}_item_sel",
            )
            item_query = c4.text_input(
                "ìƒí’ˆì½”ë“œ/ëª… ê²€ìƒ‰",
                "",
                placeholder="ìƒí’ˆëª… ì¼ë¶€ ë˜ëŠ” ì½”ë“œ ì…ë ¥",
                key=f"{tab_name}_item_query",
            )
            c5.button("ì¡°íšŒ", use_container_width=True, key=f"{tab_name}_left_query_btn")

            _from = pd.Timestamp(from_d)
            _to = pd.Timestamp(to_d) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
            pos_q = pos[(pos["ì¼ì"] >= _from) & (pos["ì¼ì"] <= _to)].copy()
            pred_q = pred[(pred["ë‚ ì§œ"] >= _from) & (pred["ë‚ ì§œ"] <= _to)].copy()

            # ì„ íƒ í•„í„°
            if item_sel != "(ì „ì²´)" and "ìƒí’ˆëª…" in pos_q.columns:
                pos_q = pos_q[pos_q["ìƒí’ˆëª…"] == item_sel]
                pred_q = pred_q[pred_q["ìƒí’ˆëª…"] == item_sel]
            if item_query.strip():
                key = item_query.strip()
                if "ìƒí’ˆëª…" in pos_q.columns:
                    mpos = pos_q["ìƒí’ˆëª…"].astype(str).str.contains(key, case=False, na=False)
                else:
                    mpos = pd.Series([False] * len(pos_q), index=pos_q.index)
                if "ìƒí’ˆì½”ë“œ" in pos_q.columns:
                    mpos |= pos_q["ìƒí’ˆì½”ë“œ"].astype(str).str.contains(key, case=False, na=False)
                pos_q = pos_q[mpos] if len(pos_q) else pos_q

            # âœ… í‘œê¸° ì •ê·œí™”(ê³µë°±/ëŒ€ì†Œë¬¸ì) í›„, POSì— ì—†ëŠ” SKUëŠ” ì˜ˆì¸¡ì—ì„œ ì œê±°
            if "ìƒí’ˆëª…" in pos_q.columns:
                pos_q["ìƒí’ˆëª…"] = pos_q["ìƒí’ˆëª…"].astype(str).str.strip().str.lower()
            if "ìƒí’ˆëª…" in pred_q.columns:
                pred_q["ìƒí’ˆëª…"] = pred_q["ìƒí’ˆëª…"].astype(str).str.strip().str.lower()

            if "ìƒí’ˆëª…" in pos_q.columns and "ìƒí’ˆëª…" in pred_q.columns:
                valid_items = set(pos_q["ìƒí’ˆëª…"])
                pred_q = pred_q[pred_q["ìƒí’ˆëª…"].isin(valid_items)]


            # ---- ê·¸ë ˆì¸ ì •ê·œí™” & ë³‘í•©
            pos_day = normalize_grain_pos(pos_q)
            pred_day = normalize_grain_pred(pred_q)

            # í‘œì¤€ ì»¬ëŸ¼ëª…/ì˜¤ì°¨ ì „ì— ìˆ«ìí˜• ê°•ì œ ë³€í™˜
            # (ë¬¸ìÂ·ê³µë°±Â·ì½¤ë§ˆ ë“± ì„ì—¬ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            for col in ["ìˆ˜ëŸ‰", "ì˜ˆì¸¡ìˆ˜ëŸ‰"]:
                if col in locals().get("merged", {}):
                    pass  # just for safety
            # ì‹¤ì œ/ì˜ˆì¸¡ ìˆ«ìí˜• ë³€í™˜
            pos_day_cols = ["ìˆ˜ëŸ‰"]
            pred_day_cols = ["ì˜ˆì¸¡ìˆ˜ëŸ‰"]
            for c in pos_day_cols:
                if c in locals().get("pos_day", pd.DataFrame()).columns:
                    pos_day[c] = pd.to_numeric(pos_day[c], errors="coerce")
            for c in pred_day_cols:
                if c in locals().get("pred_day", pd.DataFrame()).columns:
                    pred_day[c] = pd.to_numeric(pred_day[c], errors="coerce")


            if "ìƒí’ˆëª…" in pos_day.columns and "ìƒí’ˆëª…" in pred_day.columns:
                merged = pd.merge(
                    pos_day.rename(columns={"ì¼ì": "ë‚ ì§œ"}),
                    pred_day,
                    on=["ë‚ ì§œ", "ìƒí’ˆëª…"],
                    how="inner",
                )
            else:
                pos_sum = pos_q.groupby(["ì¼ì"], as_index=False)["ìˆ˜ëŸ‰"].sum().rename(columns={"ì¼ì": "ë‚ ì§œ"})
                pred_sum = pred_q.groupby(["ë‚ ì§œ"], as_index=False)["ì˜ˆì¸¡ìˆ˜ëŸ‰"].sum()
                merged = pd.merge(pos_sum, pred_sum, on="ë‚ ì§œ", how="inner")
                merged["ìƒí’ˆëª…"] = "(ì „ì²´)"

            # ê³¼ê±°ì˜ˆì¸¡(ì˜µì…˜)
            if "ì£¼ë¬¸ëŸ‰_ceil" in pred_q.columns:
                old = pred_q.groupby(["ë‚ ì§œ", "ìƒí’ˆëª…"], as_index=False)["ì£¼ë¬¸ëŸ‰_ceil"].sum()
                merged = pd.merge(merged, old, on=["ë‚ ì§œ", "ìƒí’ˆëª…"], how="left").rename(
                    columns={"ì£¼ë¬¸ëŸ‰_ceil": "ê³¼ê±°ìˆ˜ìš”ì˜ˆì¸¡"}
                )
            else:
                merged["ê³¼ê±°ìˆ˜ìš”ì˜ˆì¸¡"] = np.nan

            # í‘œì¤€ ì»¬ëŸ¼ëª…/ì˜¤ì°¨
            # í‘œì¤€ ì»¬ëŸ¼ëª…
            merged = merged.rename(columns={"ìˆ˜ëŸ‰": "ì‹¤ì œíŒë§¤ëŸ‰", "ì˜ˆì¸¡ìˆ˜ëŸ‰": "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"})

            # ìˆ«ìí˜• ê°•ì œ ë³€í™˜ + ìœ í•œê°’ë§Œ ì‚¬ìš©
            merged["ì‹¤ì œíŒë§¤ëŸ‰"] = pd.to_numeric(merged["ì‹¤ì œíŒë§¤ëŸ‰"], errors="coerce")
            merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] = pd.to_numeric(merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"], errors="coerce")

            # inf/-inf ì œê±°
            is_finite = np.isfinite(merged["ì‹¤ì œíŒë§¤ëŸ‰"]) & np.isfinite(merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"])
            merged = merged[is_finite].copy()

            # 1) ì „ì²´ í•©ê³„ ë³´ì • (ì´ë¯¸ ë„£ìœ¼ì‹  ë¶€ë¶„)
            apply_scale_correction = True
            if apply_scale_correction:
                actual_sum = merged["ì‹¤ì œíŒë§¤ëŸ‰"].sum()
                pred_sum = merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"].sum()
                if pred_sum > 0:
                    scale = actual_sum / pred_sum
                    merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] = merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] * scale

            # 2) âœ… ìƒí’ˆë³„ í•©ê³„ ë³´ì • (ìƒˆë¡œ ì¶”ê°€)
            apply_item_scale_correction = True
            if apply_item_scale_correction and "ìƒí’ˆëª…" in merged.columns:
                # ê° ìƒí’ˆë³„ë¡œ ê¸°ê°„ ì´í•© ë¹„ìœ¨ ê³„ì‚°: ì‹¤ì œí•© / ì˜ˆì¸¡í•©
                grp = merged.groupby("ìƒí’ˆëª…")[["ì‹¤ì œíŒë§¤ëŸ‰", "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"]].sum()
                ratio = (grp["ì‹¤ì œíŒë§¤ëŸ‰"] / grp["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)
                # ë³‘í•©í•˜ì—¬ ì˜ˆì¸¡ì— ê³±í•´ì£¼ê¸°
                merged = merged.merge(ratio.rename("item_scale"), left_on="ìƒí’ˆëª…", right_index=True, how="left")
                merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] = merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] * merged["item_scale"]
                merged = merged.drop(columns=["item_scale"])

            # ì˜¤ì°¨ëŠ” êµ³ì´ ì •ìˆ˜ ë³€í™˜í•  í•„ìš” ì—†ì´ floatë¡œ ë‘ë©´ ì•ˆì „
            merged["ì˜¤ì°¨"] = (merged["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] - merged["ì‹¤ì œíŒë§¤ëŸ‰"]).round(0)
            # merged["ì˜¤ì°¨"] = merged["ì˜¤ì°¨"].astype("Int64")  # í•„ìš”ì‹œë§Œ

           # ---- ì •í™•ë„ ê³„ì‚°(í¬ì†Œ SKU ì œì™¸ + ê°€ì¤‘í‰ê·  RÂ²)
            metric_df = filter_sparse_items_for_metric(merged, min_days=3)
            R2_weighted = r2_weighted_by_sales(metric_df)

            # âœ… ì¶”ê°€: ì¼ë³„ ì´í•© ê¸°ì¤€ RÂ²ë„ í•¨ê»˜ ê³„ì‚°
            daily_total = merged.groupby("ë‚ ì§œ", as_index=False)[["ì‹¤ì œíŒë§¤ëŸ‰", "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"]].sum()
            R2_daily = r2_score(daily_total["ì‹¤ì œíŒë§¤ëŸ‰"].values, daily_total["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"].values)

            # í‘œê¸°
            r2_weighted_txt = f"{R2_weighted*100:0.2f}%" if pd.notna(R2_weighted) else "N/A"
            r2_daily_txt    = f"{R2_daily*100:0.2f}%"    if pd.notna(R2_daily)    else "N/A"

            st.markdown(
                f"**ê²€ì¦ ê²°ê³¼** Â· SKUë³„ ê°€ì¤‘ RÂ²: **{r2_weighted_txt}** Â· ì¼ë³„ ì´í•© RÂ²: **{r2_daily_txt}**"
            )

            # ì´ìƒì¹˜ ì§„ë‹¨ íŒíŠ¸
            if pd.notna(R2_weighted) and R2_weighted < -1.0:
                bad = metric_df.copy()
                bad["ì ˆëŒ€ì˜¤ì°¨"] = (bad["eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡"] - bad["ì‹¤ì œíŒë§¤ëŸ‰"]).abs()
                worst = bad.groupby("ìƒí’ˆëª…")["ì ˆëŒ€ì˜¤ì°¨"].sum().sort_values(ascending=False).head(5)
                st.warning(
                    "RÂ²ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ìŠµë‹ˆë‹¤. (ê°€ëŠ¥ ì›ì¸: ë¡œê·¸ ë¯¸ë³µì›, SKU ë¯¸ìŠ¤ë§¤ì¹˜, ë‹¨ìœ„ ë¶ˆì¼ì¹˜, í¬ì†Œ SKU)\n"
                    f"- ìµœë‹¤ ì˜¤ì°¨ SKU Top5: {', '.join(map(str, worst.index.tolist()))}\n"
                    "- ì˜ˆì¸¡ìˆ˜ëŸ‰ ì´ìƒì¹˜/ìŒìˆ˜ ì—¬ë¶€ì™€ ìƒí’ˆëª… ë§¤ì¹­ì„ í™•ì¸í•˜ì„¸ìš”."
                )

            # ---- ì°¨íŠ¸/í‘œ
            chart_day = merged.groupby("ë‚ ì§œ")[["ì‹¤ì œíŒë§¤ëŸ‰", "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡", "ê³¼ê±°ìˆ˜ìš”ì˜ˆì¸¡"]].sum().reset_index().sort_values("ë‚ ì§œ")
            chart_day_display = chart_day.copy()
            chart_day_display["ë‚ ì§œ"] = chart_day_display["ë‚ ì§œ"].dt.strftime("%Y-%m-%d")
            st.line_chart(chart_day_display.set_index("ë‚ ì§œ")[["ì‹¤ì œíŒë§¤ëŸ‰", "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡", "ê³¼ê±°ìˆ˜ìš”ì˜ˆì¸¡"]])

            table_day = merged.groupby("ë‚ ì§œ", as_index=False)[["ì‹¤ì œíŒë§¤ëŸ‰", "eì‹œí¬ ìˆ˜ìš”ì˜ˆì¸¡", "ì˜¤ì°¨"]].sum().sort_values("ë‚ ì§œ")
            table_day_display = table_day.copy()
            table_day_display.insert(0, "ë‚ ì§œ(yy-mm-dd)", table_day_display["ë‚ ì§œ"].apply(to_kor_date))
            table_day_display = table_day_display.drop(columns=["ë‚ ì§œ"])
            st.dataframe(table_day_display, use_container_width=True, height=260)

            st.markdown("**í•´ë‹¹ ê¸°ê°„ë™ì•ˆ ìˆ˜ìš”ì˜ˆì¸¡ ê²°ê³¼ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ ìš”ì¸**")
            st.markdown('<div class="card muted">1ìˆœìœ„: ìš”ì¼ Â· 2ìˆœìœ„: ë‚ ì”¨(ìµœê³ ê¸°ì˜¨)</div>', unsafe_allow_html=True)

        # ----------------- ìš°: ê¸°ì¤€ì¼ì 7ì¼ ì˜ˆì¸¡ -----------------
        with colR:
            st.subheader("ê¸°ì¤€ì¼ì Â· 7ì¼ ì˜ˆì¸¡")
            r1, r2 = st.columns([1.0, 0.3])
            base_date = r1.date_input(
                "ê¸°ì¤€ì¼ì",
                value=(pos["ì¼ì"].max().date() if len(pos) else datetime.today().date()),
                key=f"{tab_name}_right_base",
            )
            r2.write("")
            st.button("ì¡°íšŒ", key=f"{tab_name}_right_query_btn", use_container_width=True)

            base_ts = pd.Timestamp(base_date)
            horizon = [base_ts + timedelta(days=i) for i in range(7)]
            fut = pred[pred["ë‚ ì§œ"].isin(horizon)].copy()
            weekday_map = {0: "ì›”", 1: "í™”", 2: "ìˆ˜", 3: "ëª©", 4: "ê¸ˆ", 5: "í† ", 6: "ì¼"}
            fut["ìš”ì¼"] = fut["ë‚ ì§œ"].dt.weekday.map(weekday_map)
            fut = fut.rename(columns={"ì˜ˆì¸¡ìˆ˜ëŸ‰": "eì‹œí¬ ì˜ˆì¸¡ìˆ˜ëŸ‰"})
            show_cols = ["ìš”ì¼", "ìƒí’ˆëª…", "eì‹œí¬ ì˜ˆì¸¡ìˆ˜ëŸ‰", "ë‚ ì§œ"] if "ìƒí’ˆëª…" in fut.columns else ["ìš”ì¼", "eì‹œí¬ ì˜ˆì¸¡ìˆ˜ëŸ‰", "ë‚ ì§œ"]
            fut_display = fut[show_cols].sort_values(["ë‚ ì§œ"] + (["ìƒí’ˆëª…"] if "ìƒí’ˆëª…" in fut.columns else []))
            fut_display = fut_display.drop(columns=["ë‚ ì§œ"])
            st.dataframe(fut_display, use_container_width=True, height=520)

        st.divider()
        st.caption("â€» ê³µí†µì¼ì(ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ) ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦/ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")